{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfacf929",
   "metadata": {},
   "source": [
    "# Boos & Stefanski (2013): M-Estimation (Estimating Equations)\n",
    "\n",
    "Here, we will implement some of the examples described in Chapter 7 of Boos & Stefanski (2013). If you have the book\n",
    "(or access to it), then reading along with each section may be helpful. Here, we code each of the estimating equations\n",
    "by-hand (rather than using the pre-built options offered).\n",
    "\n",
    "Examples of M-Estimation provided in that chapter are replicated here using `delicatessen`. Reading the chapter and looking at the corresponding implementations is likely to be the best approach to learning both the theory and application of M-Estimation. \n",
    "\n",
    "Boos DD, & Stefanski LA. (2013). M-estimation (estimating equations). In *Essential Statistical Inference* (pp. 297-337). Springer, New York, NY.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3583f2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version:        1.25.2\n",
      "SciPy version:        1.11.2\n",
      "Pandas version:       1.4.1\n",
      "Delicatessen version: 3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import delicatessen\n",
    "from delicatessen import MEstimator\n",
    "\n",
    "np.random.seed(80950841)\n",
    "\n",
    "print(\"NumPy version:       \", np.__version__)\n",
    "print(\"SciPy version:       \", sp.__version__)\n",
    "print(\"Pandas version:      \", pd.__version__)\n",
    "print(\"Delicatessen version:\", delicatessen.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2dd044",
   "metadata": {},
   "source": [
    "## 7.2 The Basic Approach\n",
    "\n",
    "### 7.2.2 Sample Mean and Variance\n",
    "The first example is the estimating equations for the mean and variance. To demonstrate the example, we will use some generic data for $Y$. Below is an example data set that will be used up to Section 7.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8a77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "data = pd.DataFrame()\n",
    "data['Y'] = np.random.normal(loc=10, scale=2, size=n)\n",
    "data['X'] = np.random.normal(loc=5, size=n)\n",
    "data['C'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e3226",
   "metadata": {},
   "source": [
    "Here, estimating equations for both the mean and variance are stacked together:\n",
    "\n",
    "$$\\psi(Y_i, \\theta) = \n",
    "\\begin{bmatrix}\n",
    "    Y_i - \\theta_1\\\\\n",
    "    (Y_i - \\theta_1)^2 - \\theta_2\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "The top estimating equation is for the mean, and the bottom estimating equation is for the (asymptotic) variance. The following is a by-hand implementation of these estimating equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49b3a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Mean & Variance\n",
      "=========================================================\n",
      "M-Estimation: by-hand\n",
      "Theta: [10.16284625  4.11208477]\n",
      "Var:  \n",
      " [[ 4.11208477 -1.6739995 ]\n",
      " [-1.6739995  36.16386927]]\n",
      "---------------------------------------------------------\n",
      "Closed-Form\n",
      "Mean:  10.162846250198633\n",
      "Var:   4.112084770881207\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_mean_var(theta):\n",
    "    \"\"\"By-hand stacked estimating equations\"\"\"\n",
    "    return (data['Y'] - theta[0],\n",
    "            (data['Y'] - theta[0])**2 - theta[1])\n",
    "\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_mean_var, init=[0, 0])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Mean & Variance\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: by-hand\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.asymptotic_variance)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Closed-Form\")\n",
    "print(\"Mean: \", np.mean(data['Y']))\n",
    "print(\"Var:  \", np.var(data['Y'], ddof=0))\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275efcf8",
   "metadata": {},
   "source": [
    "The M-Estimator solves for $\\hat{\\theta}$ via a root finding procedure given the initial values in ``init``. Since the variance must be $>0$, we provide a positive initial value. For the sandwich variance, ``delicatessen`` uses a numerical approximation procedure for the bread matrix. This is different from the closed-form variance estimator provided in Chapter 7, but both should return approximately the same answer. The advantage of the numerically approximating the derivatives is that this process can be done for arbitrary estimating functions.\n",
    "\n",
    "Notice that $\\theta_2$ also matches the first element of the (asymptotic) variance matrix. These two values should match (since they are estimating the same thing). Further, as shown the closed-form solutions for the mean and variance are equal to the estimating equation solutions.\n",
    "\n",
    "The following uses the built-in estimating equation for the mean and variance in `delicatessen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3c0818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Mean & Variance\n",
      "=========================================================\n",
      "M-Estimation: built-in\n",
      "Theta: [10.16284625  4.11208477]\n",
      "Var:  \n",
      " [[ 4.11208477 -1.6739995 ]\n",
      " [-1.6739995  36.16386927]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from delicatessen.estimating_equations import ee_mean_variance\n",
    "\n",
    "def psi_mean_var_default(theta):\n",
    "    \"\"\"Built-in stacked estimating equations\"\"\"\n",
    "    return ee_mean_variance(y=np.asarray(data['Y']), theta=theta)\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_mean_var_default, init=[0, 0])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Mean & Variance\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: built-in\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.asymptotic_variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4fb6",
   "metadata": {},
   "source": [
    "### 7.2.3 Ratio Estimator\n",
    "Now consider if we wanted to estimate the ratio between two means. One way to implement this is the following single estimating equation\n",
    "\n",
    "$$\\psi(Y_i, \\theta) = \n",
    "\\begin{bmatrix}\n",
    "    Y_i - X_i \\times \\theta_1\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "and is implemented in `delicatessen` by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Ratio Estimator\n",
      "=========================================================\n",
      "M-Estimation: single estimating equation\n",
      "Theta: [2.08234516]\n",
      "Var:   [[0.3384233]]\n",
      "---------------------------------------------------------\n",
      "Closed-Form\n",
      "Ratio: 2.0823451609959682\n",
      "Var:   0.33842329733168625\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_ratio(theta):\n",
    "    return data['Y'] - data['X']*theta\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_ratio, init=[0, ])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Ratio Estimator\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: single estimating equation\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \",estr.asymptotic_variance)\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Closed-Form\")\n",
    "\n",
    "theta = np.mean(data['Y']) / np.mean(data['X'])\n",
    "b = 1 / np.mean(data['X'])**2\n",
    "c = np.mean((data['Y'] - theta*data['X'])**2)\n",
    "var = b * c\n",
    "print(\"Ratio:\",theta)\n",
    "print(\"Var:  \",var)\n",
    "\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0236a",
   "metadata": {},
   "source": [
    "As you may notice, only a single initial value is provided (since only a single array is being returned). Furthermore,\n",
    "we provide an initial value $>0$ since we are estimating a ratio.\n",
    "\n",
    "However, there is another set of stacked estimating equations we can consider for the ratio. Specifically, we can estimate each\n",
    "of the means and then take the ratio of those means. Below is this alternative set of estimating equations\n",
    "\n",
    "$$\\psi(Y_i, \\theta) = \n",
    "\\begin{bmatrix}\n",
    "    Y_i - \\theta_1\\\\\n",
    "    X_i - \\theta_2\\\\\n",
    "    \\theta_1 - \\theta_2 \\theta_3\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "Note that the last element is the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c63d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Ratio Estimator\n",
      "=========================================================\n",
      "M-Estimation: three estimating equations\n",
      "Theta: [10.16284625  4.88048112  2.08234516]\n",
      "Var:  \n",
      " [[ 4.11208477  0.04326814  0.82409608]\n",
      " [ 0.04326814  0.95223639 -0.39742316]\n",
      " [ 0.82409608 -0.39742316  0.3384232 ]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_ratio_three(theta):\n",
    "    return (data['Y'] - theta[0],\n",
    "            data['X'] - theta[1],\n",
    "            np.ones(data.shape[0])*theta[0] - theta[1]*theta[2])\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_ratio_three, init=[0.1, 0.1, 0.1])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Ratio Estimator\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: three estimating equations\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.asymptotic_variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0c13a",
   "metadata": {},
   "source": [
    "Here, we used a trick to make sure the dimension of ``ratio`` stays as $n$, we use ``np.ones``. Without multiplying by the array of ones, ``ratio`` would be a single value. However, ``MEstimator`` expects a $3 \\times n$ array. Multiplying the 3rd equation by an array of 1's ensure the same dimension.\n",
    "\n",
    "Also notice this form requires the use of 3 ``init`` values, unlike the other ratio estimator. As before, the ratio initial value is set $>0$ to be nice to the root-finding algorithm.\n",
    "\n",
    "Finally, note that we could have also estimated the log of the ratio. This gives a third set of estimating equations we could have considered\n",
    "\n",
    "### 7.2.4 Delta Method via M-Estimation\n",
    "\n",
    "The delta method has been used in a variety of contexts, including estimating the variance for transformations of parameters. Instead of separately estimating the parameters, transforming the parameters, and then using the delta method to estimate the variance of the transformed parameters; we can apply the transformation in an estimating equation and automatically estimate the variance for the transformed parameter(s) via the sandwich variance estimator. To do this, we stack the estimating equation for the transformation into our set of estimating equations. This was already done previously for the ratio that was based on three estimating equations.\n",
    "\n",
    "Below is another example, with the mean-variance estimating equations stacked with two transformations of the variance\n",
    "\n",
    "$$\\psi(Y_i, \\theta) = \n",
    "\\begin{bmatrix}\n",
    "    Y_i - \\theta_1\\\\\n",
    "    (Y_i - \\theta_1)^2 - \\theta_2\\\\\n",
    "    \\sqrt{\\theta_2} - \\theta_3\\\\\n",
    "    \\log(\\theta_2) - \\theta_4\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "These equations can be implemented in `delicatessen` by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05e8716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Delta Method\n",
      "=========================================================\n",
      "M-Estimation\n",
      "Theta: [10.16284625  4.11208477  2.0278276   1.41393014]\n",
      "Var:  \n",
      " [[ 0.02056042 -0.00837    -0.00206379 -0.00203546]\n",
      " [-0.00837     0.18081935  0.04458452  0.04397267]\n",
      " [-0.00206379  0.04458452  0.01099318  0.01084232]\n",
      " [-0.00203546  0.04397267  0.01084232  0.01069352]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_delta(theta):\n",
    "    return (data['Y'] - theta[0],\n",
    "            (data['Y'] - theta[0])**2 - theta[1],\n",
    "            np.ones(data.shape[0])*np.sqrt(theta[1]) - theta[2],\n",
    "            np.ones(data.shape[0])*np.log(theta[1]) - theta[3])\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_delta, init=[10., 2., 1., 1.])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Delta Method\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0a530",
   "metadata": {},
   "source": [
    "Notice the use of the ``np.ones`` trick to ensure that the final equations are the correct shapes. Here, there are 4 parameters, so ``init`` must be provided 4 values.\n",
    "\n",
    "\n",
    "### 7.2.6 Instrumental Variable Estimation\n",
    "\n",
    "Consider the following instrumental variable approach to correcting for measurement error of a variable. Here, $Y$ is the outcome of interest, $X$ is the true variable, $X^*$ is the possibly mismeasured version of $X$, and $I$ is the instrument for $X$. We are interested in estimating $\\beta_1$ of\n",
    "$$Y_i = \\beta_0 + \\beta_1 X_i + e_{i,j}$$\n",
    "Since $X^*$ is potentially mismeasured, we can't directly estimate $\\beta_1$. Instead, we need to use an instrumental variable approach. Below is some generated data consistent with this measurment error story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "440d11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some data\n",
    "n = 500\n",
    "data = pd.DataFrame()\n",
    "data['X'] = np.random.normal(size=n)\n",
    "data['Y'] = 0.5 + 2*data['X'] + np.random.normal(loc=0, size=n)\n",
    "data['X-star'] = data['X'] + np.random.normal(loc=0, size=n)\n",
    "data['T'] = -0.75 - 1*data['X'] + np.random.normal(loc=0, size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8862d",
   "metadata": {},
   "source": [
    "Two variations on the estimating equations for instrumental variable analyses are provided in the book. The first estimating equation is\n",
    "$$\\psi(Y_i,X_i^*,T_i, \\theta) = \n",
    "\\begin{bmatrix}\n",
    "    T_i - \\theta_1\\\\\n",
    "    (Y_i - \\theta_2X_i^*)(\\theta_1 - T_i)\n",
    "\\end{bmatrix} $$\n",
    "where $\\theta_1$ is the mean of the instrument, and $\\theta_2$ corresponds to $\\beta_1$. The previous estimating equations can be translated as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d19de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Instrumental Variable\n",
      "=========================================================\n",
      "M-Estimation\n",
      "Theta: [-0.89989957  2.01777751]\n",
      "Var:  \n",
      " [[ 0.00430115 -0.0006694 ]\n",
      " [-0.0006694   0.023841  ]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_instrument(theta):\n",
    "    return (theta[0] - data['T'],\n",
    "            (data['Y'] - data['X-star']*theta[1])*(theta[0] - data['T']))\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_instrument, init=[0.1, 0.1])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Instrumental Variable\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6773839d",
   "metadata": {},
   "source": [
    "As mentioned in the chapter, certain joint distributions may be of interest. To capture these additional distributions,\n",
    "the estimating equations were updated to\n",
    "$$\\psi(Y_i,X_i^*,T_i, \\theta) = \n",
    "\\begin{bmatrix}\n",
    "    T_i - \\theta_1\\\\\n",
    "    \\theta_2 - X_i^* \\\\\n",
    "    (Y_i - \\theta_3 X_i^*)(\\theta_2 - X_i^*)\\\\\n",
    "    (Y_i - \\theta_4 X_i^*)(\\theta_1 - T_i)\n",
    "\\end{bmatrix} $$\n",
    "This set of estimating equations further allows for inference on the difference between $\\beta_1$ minus the coefficient for $Y$ given $X^*$. Here, $\\theta_1$ is the mean of the instrument, $\\theta_2$ is the mean of the mismeasured value of $X$, and $\\theta_3$ corresponds to the coefficient for $Y$ given $X^*$, and $\\theta_4$ is $\\beta_1$.\n",
    "\n",
    "Again, we can easily translate these equations for ``delicatessen``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc959890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Instrumental Variable\n",
      "=========================================================\n",
      "M-Estimation\n",
      "Theta: [-0.89989957  0.02117577  0.95717618  2.01777751]\n",
      "Var:  \n",
      " [[ 0.00430115 -0.00207361 -0.00011136 -0.0006694 ]\n",
      " [-0.00207361  0.0041239   0.00023703  0.00039778]\n",
      " [-0.00011136  0.00023703  0.00302462  0.00171133]\n",
      " [-0.0006694   0.00039778  0.00171133  0.023841  ]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi(theta):\n",
    "    return (theta[0] - data['T'],\n",
    "            theta[1] - data['X-star'],\n",
    "            (data['Y'] - data['X-star']*theta[2])*(theta[1] - data['X-star']),\n",
    "            (data['Y'] - data['X-star']*theta[3])*(theta[0] - data['T'])\n",
    "            )\n",
    "\n",
    "\n",
    "estr = MEstimator(psi, init=[0.1, 0.1, 0.1, 0.1])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Instrumental Variable\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6f612",
   "metadata": {},
   "source": [
    "## 7.4 Nonsmooth Estimating Functions\n",
    "\n",
    "### 7.4.1 Robust Location Estimation\n",
    "\n",
    "To begin, we generate some generic data used for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b42c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.normal(size=250)\n",
    "n = y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98a04d",
   "metadata": {},
   "source": [
    "The robust location estimator reduces the influence of outliers by applying bounds. The robust mean proposed by Huber (1964) is\n",
    "   \n",
    "$$\\psi(Y_i, \\theta_1) = g_k(Y_i) - \\theta_1$$\n",
    "\n",
    "where $k$ indicates the bound, such that if $Y_i>k$ then $k$, or $Y_i<-k$ then $-k$, otherwise $Y_i$. \n",
    "\n",
    "Below is the estimating equation translated into code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3199896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Robust Location\n",
      "=========================================================\n",
      "M-Estimation\n",
      "Theta: [0.03056108]\n",
      "Var:  \n",
      " [[0.00370521]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_robust_mean(theta):\n",
    "    k = 3                          # Bound value\n",
    "    yr = np.where(y > k, k, y)     # Applying upper bound\n",
    "    yr = np.where(y < -k, -k, y)   # Applying lower bound\n",
    "    return yr - theta\n",
    "\n",
    "\n",
    "estr = MEstimator(psi_robust_mean, init=[0.])\n",
    "estr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Robust Location\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation\")\n",
    "print(\"Theta:\", estr.theta)\n",
    "print(\"Var:  \\n\", estr.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7ab03",
   "metadata": {},
   "source": [
    "Notice that the estimating equation here is not smooth at certain points (i.e., non-differentiable at $k$).\n",
    "\n",
    "## 7.5 Regression M-estimators\n",
    "\n",
    "### 7.5.1 Linear Model with Random $X$\n",
    "\n",
    "For the next examples, the following simulated data is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "592d77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "data = pd.DataFrame()\n",
    "data['X'] = np.random.normal(size=n)\n",
    "data['Z'] = np.random.normal(size=n)\n",
    "data['Y'] = 0.5 + 2*data['X'] - 1*data['Z'] + np.random.normal(size=n)\n",
    "data['C'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce79570",
   "metadata": {},
   "source": [
    "Here, we are interested in estimating the relationship between $Y$ and $X,Z$. We will do this via linear regression. Note that we need to manually add an intercept (the column `C` in the data). \n",
    "\n",
    "It is also worthwhile to note that the variance here is robust (to violations of the homoscedastic assumption). As comparison, we provide the equivalent using `statsmodels` generalized linear model with heteroscedastic-corrected variances.\n",
    "\n",
    "As with all the preceding estimating equations, there are multiple ways to code these. Since linear regression involves\n",
    "matrix manipulations for the programmed estimating equations to return the correct format for ``delicatessen``, we\n",
    "highlight two variations here.\n",
    "\n",
    "First, we present a for-loop implementation of the estimating equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bc3ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Linear Model\n",
      "=========================================================\n",
      "M-Estimation: by-hand\n",
      "[ 0.41082601  1.96289222 -1.02663555]\n",
      "[[ 2.18524093e-03  7.28170086e-05  1.54216639e-04]\n",
      " [ 7.28170086e-05  2.08315701e-03 -4.09519515e-05]\n",
      " [ 1.54216639e-04 -4.09519515e-05  2.14573774e-03]]\n",
      "---------------------------------------------------------\n",
      "GLM Estimator\n",
      "[ 0.41082601  1.96289222 -1.02663555]\n",
      "[[ 2.18524092e-03  7.28169947e-05  1.54216630e-04]\n",
      " [ 7.28169947e-05  2.08315690e-03 -4.09519947e-05]\n",
      " [ 1.54216630e-04 -4.09519947e-05  2.14573770e-03]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi(theta):\n",
    "    # Transforming to arrays\n",
    "    x = np.asarray(data[['C', 'X', 'Z']])\n",
    "    y = np.asarray(data['Y'])\n",
    "    beta = np.asarray(theta)[:, None]\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Where to store each of the resulting estimates\n",
    "    est_vals = []\n",
    "\n",
    "    # Looping through each observation\n",
    "    for i in range(n):\n",
    "        v_i = (y[i] - np.dot(x[i], beta)) * x[i]\n",
    "        est_vals.append(v_i)\n",
    "\n",
    "    # returning 3-by-n object\n",
    "    return np.asarray(est_vals).T\n",
    "\n",
    "mestimator = MEstimator(psi, init=[0.1, 0.1, 0.1])\n",
    "mestimator.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Linear Model\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: by-hand\")\n",
    "print(mestimator.theta)\n",
    "print(mestimator.variance)\n",
    "print(\"---------------------------------------------------------\")\n",
    "\n",
    "print(\"GLM Estimator\")\n",
    "glm = smf.glm(\"Y ~ X + Z\", data).fit(cov_type=\"HC1\")\n",
    "print(np.asarray(glm.params))\n",
    "print(np.asarray(glm.cov_params()))\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbf9f2",
   "metadata": {},
   "source": [
    "As the second approach, a vectorized version is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be4b0ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Linear Model\n",
      "=========================================================\n",
      "M-Estimation: by-hand\n",
      "[ 0.41082601  1.96289222 -1.02663555]\n",
      "[[ 2.18524113e-03  7.28169589e-05  1.54216655e-04]\n",
      " [ 7.28169589e-05  2.08315685e-03 -4.09520174e-05]\n",
      " [ 1.54216655e-04 -4.09520174e-05  2.14573766e-03]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_regression(theta):\n",
    "    x = np.asarray(data[['C', 'X', 'Z']])\n",
    "    y = np.asarray(data['Y'])[:, None]\n",
    "    beta = np.asarray(theta)[:, None]\n",
    "    return ((y - np.dot(x, beta)) * x).T\n",
    "\n",
    "\n",
    "mestimator = MEstimator(psi_regression, init=[0.1, 0.1, 0.1])\n",
    "mestimator.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Linear Model\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: by-hand\")\n",
    "print(mestimator.theta)\n",
    "print(mestimator.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8774c",
   "metadata": {},
   "source": [
    "While these two approaches give the same answer, vectorized versions will generally be faster than for-loop variations (but may be less 'human readable'). Having said that, it is easier to make a mistake with a vectorized version. We would generally recommend creating a for-loop version first (and then creating a vectorized version if that for-loop is too slow).\n",
    "\n",
    "The following uses the built-in linear regression functionality (which uses a vectorized implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4762b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Linear Model\n",
      "=========================================================\n",
      "M-Estimation: built-in\n",
      "[ 0.41082601  1.96289222 -1.02663555]\n",
      "[[ 2.18524113e-03  7.28169589e-05  1.54216655e-04]\n",
      " [ 7.28169589e-05  2.08315685e-03 -4.09520174e-05]\n",
      " [ 1.54216655e-04 -4.09520174e-05  2.14573766e-03]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from delicatessen.estimating_equations import ee_regression\n",
    "\n",
    "def psi_regression(theta):\n",
    "    return ee_regression(theta=theta,\n",
    "                         X=data[['C', 'X', 'Z']],\n",
    "                         y=data['Y'],\n",
    "                         model='linear')\n",
    "\n",
    "\n",
    "mestimator = MEstimator(psi_regression, init=[0.1, 0.1, 0.1])\n",
    "mestimator.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Linear Model\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: built-in\")\n",
    "print(mestimator.theta)\n",
    "print(mestimator.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b41d09",
   "metadata": {},
   "source": [
    "### 7.5.4 Robust Regression\n",
    "\n",
    "The next example is robust regression, where the standard linear regression model is made robust to outliers.\n",
    "We use :math:`f_k()` from 7.4.1 but now apply it to the residuals of the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dd6f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Robust Linear Model\n",
      "=========================================================\n",
      "M-Estimation: by-hand\n",
      "[ 0.41223641  1.95577495 -1.02508413]\n",
      "[[ 2.31591830e-03  1.82105969e-04  2.57209766e-04]\n",
      " [ 1.82105969e-04  2.12098818e-03 -6.95783670e-05]\n",
      " [ 2.57209766e-04 -6.95783670e-05  2.38212585e-03]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi_robust_regression(theta):\n",
    "    k = 1.345    \n",
    "    x = np.asarray(data[['C', 'X', 'Z']])\n",
    "    y = np.asarray(data['Y'])[:, None]\n",
    "    beta = np.asarray(theta)[:, None]\n",
    "    preds = np.clip(y - np.dot(x, beta), -k, k)\n",
    "    return (preds * x).T\n",
    "\n",
    "\n",
    "mestimator = MEstimator(psi_robust_regression, init=[0.5, 2., -1.])\n",
    "mestimator.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Robust Linear Model\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: by-hand\")\n",
    "print(mestimator.theta)\n",
    "print(mestimator.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853cbf2",
   "metadata": {},
   "source": [
    "The following uses the built-in robust linear regression functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84eea92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Robust Linear Model\n",
      "=========================================================\n",
      "M-Estimation: built-in\n",
      "[ 0.41223641  1.95577495 -1.02508413]\n",
      "[[ 2.31591830e-03  1.82105969e-04  2.57209766e-04]\n",
      " [ 1.82105969e-04  2.12098818e-03 -6.95783670e-05]\n",
      " [ 2.57209766e-04 -6.95783670e-05  2.38212585e-03]]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from delicatessen.estimating_equations import ee_robust_regression\n",
    "\n",
    "def psi_robust_regression(theta):\n",
    "    return ee_robust_regression(theta=theta,\n",
    "                                X=data[['C', 'X', 'Z']],\n",
    "                                y=data['Y'],\n",
    "                                model='linear',\n",
    "                                loss='huber', k=1.345)\n",
    "\n",
    "mestimator = MEstimator(psi_robust_regression, init=[0.5, 2., -1.])\n",
    "mestimator.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Robust Linear Model\")\n",
    "print(\"=========================================================\")\n",
    "print(\"M-Estimation: built-in\")\n",
    "print(mestimator.theta)\n",
    "print(mestimator.variance)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567903cc",
   "metadata": {},
   "source": [
    "You'll notice that the coefficients have changed slightly here. That is because we have reduced the extent of outliers\n",
    "on the estimation of the linear regression parameters (however, our simulated data mechanism doesn't really result in\n",
    "major outliers, so the change is small here).\n",
    "\n",
    "### 7.5.5 Generalized Linear Models\n",
    "\n",
    "The last category of models consider here are generalized linear models (GLMs). These models are much more flexible than the previous regression models in terms of the distributions being assumed. To illustrate this, we will use GLMs to estimate the odds ratio, risk ratio, and risk difference (since the book does not provide specific examples).\n",
    "\n",
    "The following is some generic data for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31237839",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame()\n",
    "d['X'] = [1, -1, 0, 1, 2, 1, -2, -1, 0, 3, -3, 1, 1, -1, -1, -2, 2, 0, -1, 0]\n",
    "d['Z'] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "d['Y'] = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
    "d['I'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb98742",
   "metadata": {},
   "source": [
    "For the subsequent examples, we will use the built-in GLM functionality. \n",
    "\n",
    "#### Odds Ratio\n",
    "\n",
    "To estimate odds ratios, we will use logistic regression. Logistic regression can be implemented through a GLM with a binomial distribution and the logit link. Below is the implemenentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89ed3a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Logistic\n",
      "=========================================================\n",
      "ln(OR): [-0.34613077  0.16182415  0.28150058]\n",
      "OR:     [0.70741997 1.17565348 1.32511677]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "from delicatessen.estimating_equations import ee_glm\n",
    "\n",
    "def psi(theta):\n",
    "    return ee_glm(theta, X=d[['I', 'X', 'Z']], y=d['Y'],\n",
    "                  distribution='binomial', link='logit')\n",
    "\n",
    "mestr = MEstimator(psi, init=[0., 0., 0.])\n",
    "mestr.estimate()\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Logistic\")\n",
    "print(\"=========================================================\")\n",
    "print(\"ln(OR):\", mestr.theta)\n",
    "print(\"OR:    \", np.exp(mestr.theta))\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb7b12",
   "metadata": {},
   "source": [
    "To estimate the risk ratios, we can use a log-binomial regression model. This can be implemented through a GLM with the binomial distribution and the log identity. Below is code to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd8fba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Log-binomial\n",
      "=========================================================\n",
      "ln(RR): [-0.89140821  0.06939184  0.16163837]\n",
      "RR:     [0.41007787 1.07185612 1.1754351 ]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi(theta):\n",
    "    return ee_glm(theta, X=d[['I', 'X', 'Z']], y=d['Y'],\n",
    "                  distribution='binomial', link='log')\n",
    "\n",
    "mestr = MEstimator(psi, init=[-.9, 0., 0.])\n",
    "mestr.estimate(solver='lm')\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Log-binomial\")\n",
    "print(\"=========================================================\")\n",
    "print(\"ln(RR):\", mestr.theta)\n",
    "print(\"RR:    \", np.exp(mestr.theta))\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0875166f",
   "metadata": {},
   "source": [
    "As seen here, the odds ratio and risk ratios differ. This is expected since the outcome is not that rare.\n",
    "\n",
    "Note: the log-binomial can be a bit difficult to estimate as it is not bounded like the risk ratio is. What this means is that decent starting values might need to be provided. As seen here, the intercept is given a good starting value.\n",
    "\n",
    "Finally, we can estimate the risk difference via linear-binomial regression. This can be done with the binomial distribution but with the identity link. Below is code for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf03ae21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================\n",
      "Risk difference\n",
      "=========================================================\n",
      "RD:    [0.41621376 0.03456562 0.06753619]\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "def psi(theta):\n",
    "    return ee_glm(theta, X=d[['I', 'X', 'Z']], y=d['Y'],\n",
    "                  distribution='binomial', link='identity')\n",
    "\n",
    "mestr = MEstimator(psi, init=[.2, 0., 0.])\n",
    "mestr.estimate(solver='lm')\n",
    "\n",
    "print(\"=========================================================\")\n",
    "print(\"Risk difference\")\n",
    "print(\"=========================================================\")\n",
    "print(\"RD:   \", mestr.theta)\n",
    "print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3678fe15",
   "metadata": {},
   "source": [
    "Like the log-binomial model this too can be difficult to fit since the risk differences are bounded but the model is not. This is especially a concern for the linear-binomial model, since we need to ensure the starting values don't produce risk differences outside the range of $[-1,1]$. To ensure this, a starting value of $0.2$ was used for the intercept.\n",
    "\n",
    "This concludes the replication of the examples provided in Boos & Stefanski (2002). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
